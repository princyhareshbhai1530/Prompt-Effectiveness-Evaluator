# -*- coding: utf-8 -*-
"""Prompt-Effectiveness-Evaluator.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/124Re0N9-MD5pIflhfCtKyfi6gjeG1TD4
"""

# Commented out IPython magic to ensure Python compatibility.
# Step 1 ‚Äî install deps (pinned) and auto-restart. DO NOT import numpy/pandas before restart.
# %pip -q install --upgrade pip
# %pip -q install "numpy==1.26.4" "pandas==2.2.2" "streamlit==1.37.1" \
                "plotly==5.22.0" "sacrebleu==2.4.0" "rouge-score==0.1.2" \
                "bert-score==0.3.13" "transformers==4.41.0" \
                "openai==1.35.10" "scikit-learn==1.5.1" torch -U

# Cloudflared for public URL
!wget -q -O cloudflared-linux-amd64.deb https://github.com/cloudflare/cloudflared/releases/latest/download/cloudflared-linux-amd64.deb
!dpkg -i cloudflared-linux-amd64.deb > /dev/null 2>&1 || true

import os
print("üîÅ Restarting runtime now so new binaries are used‚Ä¶")
os.kill(os.getpid(), 9)

from pathlib import Path
import pandas as pd

base = Path('/content/PromptEffectivenessEvaluator')
mods = base/'modules'
data = base/'data'
mods.mkdir(parents=True, exist_ok=True)
data.mkdir(parents=True, exist_ok=True)

# ----------------- app.py -----------------
app_code = """
import os, json, random
import pandas as pd
import streamlit as st

from modules.llm_query import generate_with_prompt, ModelCallError
from modules.evaluation import evaluate_outputs, safe_float
from modules.visualization import plot_metric_bar, plot_radar

st.set_page_config(page_title="Prompt Effectiveness Evaluator", layout="wide")
st.title("üß™ Prompt Effectiveness Evaluator")
st.caption("Compare multiple prompt variants for the same task with ROUGE/BLEU/BERTScore and sentiment metrics.")

def sales_projection_demo():
    csv_path = os.path.join("data","sales_data.csv")
    if os.path.exists(csv_path):
        df = pd.read_csv(csv_path)
        months = df["Month"].tolist()
        sales = df["Sales"].tolist()
    else:
        months = ["Jan","Feb","Mar","Apr","May","Jun","Jul","Aug","Sep"]
        sales = [10000,10500,11000,11500,11800,12000,12500,13000,13500]

    growth = ((sales[-1] / sales[-4]) ** (1/3)) - 1
    o = sales[-1]*(1+growth); n = o*(1+growth); d = n*(1+growth)

    source = ("The company's sales figures for the year so far are as follows:\\n" +
              ", ".join(f"{m}: ${v}" for m,v in zip(months,sales)) +
              ". Based on this trend, estimate the projected sales for October, November, and December.")
    reference = (f"Projected sales based on recent growth trends: "
                 f"October: ${o:,.0f}, November: ${n:,.0f}, December: ${d:,.0f}.")
    return source, reference

with st.sidebar:
    st.header("‚öôÔ∏è Settings")
    api_key = st.text_input("OpenAI API Key (optional)", type="password") or os.environ.get("OPENAI_API_KEY","")
    model = st.selectbox("Model", ["gpt-4o-mini", "gpt-4o", "gpt-4.1-mini", "gpt-4.1"], index=0)
    temperature = st.slider("Temperature", 0.0, 1.5, 0.3, 0.1)
    top_p = st.slider("Top_p", 0.0, 1.0, 1.0, 0.05)
    max_tokens = st.number_input("Max tokens", min_value=64, max_value=4096, value=512, step=64)
    use_bertscore = st.checkbox("Compute BERTScore (slower)", value=False)

    st.divider()
    st.subheader("üíæ Import/Export")
    up = st.file_uploader("Load prompt variants JSON", type=["json"])
    if up:
        try:
            payload = json.load(up)
            st.session_state["prompt_variants"] = payload.get("prompt_variants", [])
            st.success(f"Loaded {len(st.session_state['prompt_variants'])} variants.")
        except Exception as e:
            st.error(f"Failed to parse JSON: {e}")

col1,col2 = st.columns([2,1])
with col1:
    task_type = st.selectbox("Task type",
        ["Summarization","Q&A","Sentiment Rephrasing","Sales Projection","General Generation"], index=0)
with col2:
    n = st.number_input("Responses per prompt (n)", 1, 5, 1, 1)

st.write("### Input")
if "source_text" not in st.session_state: st.session_state["source_text"] = ""
if "reference_text" not in st.session_state: st.session_state["reference_text"] = ""

if task_type == "Sales Projection" and st.button("üìà Load Sales Projection Demo"):
    s,r = sales_projection_demo()
    st.session_state["source_text"] = s
    st.session_state["reference_text"] = r

if task_type == "Summarization":
    source_text = st.text_area("Source text", key="source_text", height=180)
    reference_text = st.text_area("Reference summary (optional)", key="reference_text", height=120)
elif task_type == "Q&A":
    source_text = st.text_area("Knowledge/context", key="source_text", height=160)
    reference_text = st.text_input("Target answer (optional)", key="reference_text")
elif task_type == "Sentiment Rephrasing":
    source_text = st.text_area("Original text", key="source_text", height=160)
    reference_text = st.selectbox("Target sentiment", ["","positive","negative","neutral"], index=0, key="reference_text")
elif task_type == "Sales Projection":
    source_text = st.text_area("Sales data / instruction text", key="source_text", height=160)
    reference_text = st.text_area("Reference projection (optional)", key="reference_text", height=120)
else:
    source_text = st.text_area("Input / Instructions", key="source_text", height=160)
    reference_text = st.text_area("Reference output (optional)", key="reference_text", height=120)

st.write("### Prompt Variants")
if "prompt_variants" not in st.session_state:
    st.session_state["prompt_variants"] = [
        {"label":"Zero-shot concise","prompt":"Summarize the following text in 3 crisp bullet points:\\n\\n{source}"},
        {"label":"Role + constraints","prompt":"You are a senior editor. Produce an accurate, neutral 3-sentence summary of the text.\\n\\nTEXT:\\n{source}"},
        {"label":"CoT reasoning","prompt":"Think step-by-step about the core claims and evidence in the text, then write a faithful 4-sentence summary.\\n\\nCONTENT:\\n{source}"}
    ]

edited=[]
for i,item in enumerate(st.session_state["prompt_variants"]):
    with st.expander(f"Variant {i+1}: {item.get('label','(untitled)')}"):
        lbl=st.text_input("Label",value=item.get("label",""),key=f"lbl_{i}")
        prm=st.text_area("Prompt template (use {source})",value=item.get("prompt",""),key=f"pr_{i}",height=140)
        edited.append({"label":lbl,"prompt":prm})
st.session_state["prompt_variants"]=edited

if st.button("üöÄ Run Evaluation"):
    if not st.session_state["prompt_variants"]:
        st.error("Please add at least one prompt variant.")
    else:
        outputs=[]
        with st.spinner("Generating outputs..."):
            for var in st.session_state["prompt_variants"]:
                template=var["prompt"] or "{source}"
                filled=template.replace("{source}",source_text or "")
                try:
                    gens=generate_with_prompt(api_key=api_key,model=model,prompt=filled,n=n,
                                              temperature=safe_float(temperature),top_p=safe_float(top_p),
                                              max_tokens=int(max_tokens))
                except ModelCallError as e:
                    st.error(f"Model error for '{var['label']}': {e}")
                    gens=[f"[MOCK OUTPUT] {var['label']} -> {filled[:80]}..."]
                for g in gens:
                    outputs.append({"variant":var["label"],"prompt":template,"filled_prompt":filled,"output":g})
        df=pd.DataFrame(outputs)
        st.dataframe(df,use_container_width=True)

        st.subheader("üìè Metrics")
        metrics=evaluate_outputs(df=df,task_type=task_type,reference=reference_text,use_bertscore=use_bertscore)
        st.dataframe(metrics,use_container_width=True)

        st.subheader("üìä Visualizations")
        st.plotly_chart(plot_metric_bar(metrics,"rougeL"),use_container_width=True)
        st.plotly_chart(plot_metric_bar(metrics,"bleu"),use_container_width=True)
        if "bertscore_f1" in metrics.columns:
            st.plotly_chart(plot_metric_bar(metrics,"bertscore_f1"),use_container_width=True)
        st.plotly_chart(plot_metric_bar(metrics,"sentiment_conf"),use_container_width=True)
        st.plotly_chart(plot_radar(metrics),use_container_width=True)

        st.download_button("‚¨áÔ∏è Download metrics (CSV)",data=metrics.to_csv(index=False),
                           file_name="metrics.csv",mime="text/csv")
        st.download_button("‚¨áÔ∏è Download raw outputs (JSON)",data=df.to_json(orient="records",indent=2),
                           file_name="outputs.json",mime="application/json")
"""
(base/'app.py').write_text(app_code)

# ----------------- llm_query.py -----------------
llm_code = """
import random
from typing import List, Optional

class ModelCallError(Exception):
    pass

def _mock_outputs(prompt: str, n: int) -> List[str]:
    random.seed(len(prompt))
    stems = ["Summary:", "Answer:", "Key Points:", "Rephrased:", "Findings:",
             "Insight:", "Conclusion:", "Response:"]
    return [f"{random.choice(stems)} {prompt[:120]} ... (mock)" for _ in range(n)]

def generate_with_prompt(api_key: Optional[str], model: str, prompt: str, n: int = 1,
                         temperature: float = 0.3, top_p: float = 1.0,
                         max_tokens: int = 512) -> List[str]:
    if not api_key:
        return _mock_outputs(prompt, n)
    try:
        from openai import OpenAI
        client = OpenAI(api_key=api_key)
        resp = client.chat.completions.create(
            model=model,
            messages=[{"role":"user","content":prompt}],
            n=n, temperature=temperature, top_p=top_p, max_tokens=max_tokens,
        )
        return [c.message.content for c in resp.choices]
    except Exception as e:
        raise ModelCallError(str(e))
"""
(mods/'llm_query.py').write_text(llm_code)

# ----------------- evaluation.py -----------------
eval_code = """
from typing import List, Dict, Optional
import numpy as np
import pandas as pd

def safe_float(x):
    try:
        return float(x)
    except Exception:
        return 0.0

def _rouge_l(hyp: str, ref: str) -> float:
    try:
        from rouge_score import rouge_scorer
        scorer = rouge_scorer.RougeScorer(['rougeL'], use_stemmer=True)
        scores = scorer.score(ref, hyp)
        return scores['rougeL'].fmeasure
    except Exception:
        return np.nan

def _bleu(hyps: List[str], refs: List[str]) -> float:
    try:
        import sacrebleu
        bleu = sacrebleu.corpus_bleu(hyps, [refs])
        return bleu.score / 100.0
    except Exception:
        return np.nan

def _bertscore(hyps: List[str], refs: List[str]) -> Dict[str, float]:
    try:
        from bert_score import score
        P, R, F1 = score(hyps, refs, lang="en", verbose=False)
        return {"bertscore_p": float(P.mean()), "bertscore_r": float(R.mean()), "bertscore_f1": float(F1.mean())}
    except Exception:
        return {}

def _sentiment_score(texts: List[str]) -> List[Dict[str, float]]:
    try:
        from transformers import pipeline
        clf = pipeline("sentiment-analysis")
        res = clf(texts, truncation=True)
        out = []
        for r in res:
            label = r.get("label","").lower()
            score = float(r.get("score",0.0))
            conf = score
            sgn = 0.0
            if "pos" in label: sgn = 1.0
            elif "neg" in label: sgn = -1.0
            out.append({"sentiment_label": label, "sentiment_conf": conf, "sentiment_signed": sgn * conf})
        return out
    except Exception:
        return [{"sentiment_label":"", "sentiment_conf":np.nan, "sentiment_signed":np.nan} for _ in texts]

def evaluate_outputs(df: pd.DataFrame, task_type: str, reference: Optional[str], use_bertscore: bool=False) -> pd.DataFrame:
    rows = []
    for variant, group in df.groupby("variant"):
        outs = group["output"].astype(str).tolist()
        rougeL = np.nan; bleu = np.nan
        if reference and len(reference.strip())>0:
            rouge_vals = [_rouge_l(o, reference) for o in outs]
            rougeL = float(np.nanmean(rouge_vals))
            bleu = _bleu(outs, [reference]*len(outs))
        bert = {}
        if use_bertscore and reference and len(reference.strip())>0:
            bert = _bertscore(outs, [reference]*len(outs))
        senti = _sentiment_score(outs)
        senti_conf = float(np.nanmean([s["sentiment_conf"] for s in senti]))
        senti_signed = float(np.nanmean([s["sentiment_signed"] for s in senti]))
        row = {"variant": variant, "count": len(outs), "rougeL": rougeL, "bleu": bleu,
               "sentiment_conf": senti_conf, "sentiment_signed": senti_signed}
        row.update(bert)
        rows.append(row)
    order = [c for c in ["rougeL","bleu","bertscore_f1","sentiment_conf"] if rows and c in rows[0]]
    return pd.DataFrame(rows).sort_values(by=order, ascending=False, na_position="last")
"""
(mods/'evaluation.py').write_text(eval_code)

# ----------------- visualization.py -----------------
viz_code = """
import pandas as pd
import plotly.express as px
import plotly.graph_objects as go

def plot_metric_bar(df: pd.DataFrame, metric: str):
    if metric not in df.columns:
        return go.Figure()
    fig = px.bar(df, x="variant", y=metric, title=f"{metric} by variant")
    fig.update_layout(xaxis_title="", yaxis_title=metric)
    return fig

def plot_radar(df: pd.DataFrame):
    cols = [c for c in ["rougeL","bleu","bertscore_f1","sentiment_conf"] if c in df.columns]
    if not cols:
        return go.Figure()
    fig = go.Figure()
    for _, row in df.iterrows():
        fig.add_trace(go.Scatterpolar(
            r=[row.get(c,0) or 0 for c in cols],
            theta=cols,
            fill='toself',
            name=row["variant"]
        ))
    fig.update_layout(polar=dict(radialaxis=dict(visible=True)), showlegend=True, title="Radar: normalized metrics")
    return fig
"""
(mods/'visualization.py').write_text(viz_code)

# ----------------- sales_data.csv -----------------
sales_data = pd.DataFrame({
    "Month": ["Jan","Feb","Mar","Apr","May","Jun","Jul","Aug","Sep"],
    "Sales": [10000,10500,11000,11500,11800,12000,12500,13000,13500]
})
sales_data.to_csv(data/'sales_data.csv', index=False)

print("‚úÖ All project files and CSV created")

import subprocess, time, re, sys

base = "/content/PromptEffectivenessEvaluator"
log_path = "/content/streamlit.log"

print("üöÄ Starting Streamlit server...")
st_proc = subprocess.Popen(
    ["streamlit", "run", "app.py", "--server.port=8501", "--server.address=0.0.0.0"],
    cwd=base, stdout=open(log_path,"w"), stderr=subprocess.STDOUT, text=True
)

for i in range(5):
    time.sleep(1)
    print(f"  ‚è≥ Waiting for Streamlit... {i+1}/5 sec")

print("üåê Starting Cloudflared tunnel...")
cf_proc = subprocess.Popen(
    ["cloudflared", "tunnel", "--url", "http://localhost:8501", "--no-autoupdate"],
    stdout=subprocess.PIPE, stderr=subprocess.STDOUT, text=True
)

public_url = None
for sec in range(90):
    line = cf_proc.stdout.readline()
    if line:
        sys.stdout.write("   " + line)
        sys.stdout.flush()
        if "trycloudflare.com" in line:
            m = re.search(r"https://[a-z0-9-]+\\.trycloudflare\\.com", line)
            if m:
                public_url = m.group(0)
                break
    else:
        time.sleep(1)

print("\n==============================")
if public_url:
    print("‚úÖ Your app is live at:\n", public_url)
else:
    print("‚ùå Could not get a public URL. Check logs:")
    with open(log_path) as f:
        print(f.read()[-2000:])

