# ğŸ§ª Prompt Effectiveness Evaluator

## ğŸ“Œ Overview
This project implements a **Prompt Effectiveness Evaluator** â€” an interactive framework to **compare multiple prompt engineering strategies** (Zero-shot, Role-based, Chain-of-Thought, etc.) across tasks such as:
- Summarization
- Q&A
- Sentiment Rephrasing
- Sales Projection (with CSV demo support)
- General Text Generation

It provides **objective evaluation metrics** (ROUGE-L, BLEU, BERTScore, Sentiment Confidence) and **visualizations** (bar charts, radar plots) to analyze how different prompts impact output quality.

---

## ğŸš€ Features
- ğŸ“‚ Support for multiple **task types**
- ğŸ“ Manage and test **custom prompt variants**
- ğŸ¤– **LLM integration** (GPT-4 family: `gpt-4o`, `gpt-4o-mini`, `gpt-4.1`, `gpt-4.1-mini`)
- âš¡ **Mock mode** (runs without API key, for demo/reproducibility)
- ğŸ“Š Metrics: ROUGE-L, BLEU, optional BERTScore, Sentiment
- ğŸ“ˆ Visualizations: Bar charts + Radar chart
- ğŸ“¥ Upload and evaluate with your own **CSV data**
- â¬‡ï¸ Download raw outputs & metrics (CSV/JSON)

---

## ğŸ—ï¸ Project Structure
-PromptEffectivenessEvaluator/

â”€â”€ Prompt-Effectiveness-Evaluator.py # Streamlit main app

â”€â”€ modules/
 
 â”œâ”€â”€ llm_query.py # Handles model calls (GPT or mock)[Code included in main python file]

 â”œâ”€â”€ evaluation.py # Computes metrics (ROUGE, BLEU, etc.)[Code included in main python file]

 â”œâ”€â”€ visualization.py # Generates plots (Plotly)[Code included in main python file]

â”€â”€ data/

 â”œâ”€â”€ sample_prompts.json

 â”œâ”€â”€ sales_data.csv # Example sales dataset

---

## ğŸ“‘ Research Paper
A detailed **research paper** related to this project is also attached in the repository:  
**Prompt Effectiveness Evaluator: Analyzing the Impact of Prompt Engineering on LLM Outputs**  

This paper covers:
- Theoretical background of prompt engineering
- Experimental setup and evaluation metrics
- Sales Projection case study
- Insights and results discussion

---

## ğŸ“š References
[1] A. Shukla, *Prompt Effectiveness Evaluator: Analyzing the Impact of Prompt Engineering on LLM Outputs*, 2025. (Attached in this repository)
